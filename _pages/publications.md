---
title: "Publications"
permalink: /publications/
layout: single
comments: false
---

## Accepted Papers

<span style="font-size:60%">**[13] Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning** </span>  
<span style="font-size:60%">Sangmook Lee, Dohyung Kim, Hyukhun Koh, <u>Nakyeong Yang</u>, Kyomin Jung  
[AAAI 2026](https://aaai.org/conference/aaai/aaai-26/)</span>  

<span style="font-size:60%">**[12] Persona is a Double-edged Sword: Mitigating the Negative Impact of Role-playing Prompts in Zero-shot Reasoning Tasks** [[pdf]](https://arxiv.org/abs/2408.08631)</span>  
<span style="font-size:60%">Junseok Kim, <u>Nakyeong Yang</u>, Kyomin Jung  
[Findings of the Association for Computational Linguistics: IJCNLP-AACL 2025](https://2025.aaclnet.org/)</span>  

<span style="font-size:60%">**[11] FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge** [[pdf]](https://arxiv.org/abs/2502.19207)</span>  
<span style="font-size:60%"><u>Nakyeong Yang</u>, Minsung Kim, Seunghyun Yoon, Joongbo Shin, Kyomin Jung  
[EMNLP 2025](https://2025.emnlp.org)</span>  
<a href="https://research.adobe.com/publication/faithun-toward-faithful-forgetting-in-language-models-by-investigating-the-interconnectedness-of-knowledge/" style="font-size:60%">Joint work with Adobe Research</a>

<span style="font-size:60%">**[10] Avoidance Decoding for Diverse Multi-Branch Story Generation**</span>  
<span style="font-size:60%">Kyeongman Park, <u>Nakyeong Yang</u>, Kyomin Jung  
[EMNLP 2025](https://2025.emnlp.org)</span>

<span style="font-size:60%">**[9] Unplug and Play Language Models: Decomposing Experts in Language Models at Inference Time** [[pdf]](https://arxiv.org/abs/2404.11916)</span>  
<span style="font-size:60%"><u>Nakyeong Yang</u>, Jiwon Moon, Junseok Kim, Yunah Jang, Kyomin Jung  
[CIKM 2025](https://cikm2025.org)</span>

<span style="font-size:60%">**[8] MVMR: A New Framework for Evaluating Faithfulness of Video Moment Retrieval against Multiple Distractors** [[pdf]](https://dl.acm.org/doi/10.1145/3627673.3679838)</span>  
<span style="font-size:60%"><u>Nakyeong Yang</u>, Minsung Kim, Seunghyun Yoon, Joongbo Shin, Kyomin Jung  
[CIKM 2024 (Oral)](https://cikm2024.org/)</span>  
<a href="https://research.adobe.com/publication/mvmr-a-new-framework-for-evaluating-faithfulness-of-video-moment-retrieval-against-multiple-distractors/" style="font-size:60%">Joint work with Adobe Research</a>

<span style="font-size:60%">**[7] Mitigating Biases for Instruction-following Language Models via Bias Neurons Elimination** [[pdf]](https://aclanthology.org/2024.acl-long.490/)</span>  
<span style="font-size:60%"><u>Nakyeong Yang</u>, Taegwan Kang, Jungkyu Choi, Honglak Lee, Kyomin Jung  
[ACL 2024](https://2024.aclweb.org/)</span>  
<a href="https://www.lgresearch.ai/publication/view?seq=110" style="font-size:60%">Internship at LG AI Research</a>

<span style="font-size:60%">**[6] LongStory: Coherent, Complete and Length Controlled Long story Generation** [[pdf]](https://arxiv.org/abs/2311.15208)</span>  
<span style="font-size:60%">Kyeongman Park, <u>Nakyeong Yang</u>, Kyomin Jung  
[PAKDD 2024 (Oral)](https://pakdd2024.org/)

<span style="font-size:60%">**[5] Task-specific Compression for Multi-task Language Models using Attribution-based Pruning** [[pdf]](https://aclanthology.org/2023.findings-eacl.43/)</span>  
<span style="font-size:60%"><u>Nakyeong Yang</u>, Yunah Jang, Hwanhee Lee, Seohyeong Jeong, Kyomin Jung  
[Findings of the Association for Computational Linguistics: EACL 2023](https://2023.eacl.org/)

<span style="font-size:60%">**[4] Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer** [[pdf]](https://arxiv.org/abs/2303.13099)</span>  
<span style="font-size:60%">Hyukhun Koh, Haesung Pyun, <u>Nakyeong Yang</u>, Kyomin Jung  
[SIGDial 2023 Workshop (DSTC 11)](https://dstc11.dstc.community/)

<span style="font-size:60%">**[3] Deriving Explainable Discriminative Attributes Using Confusion About Counterfactual Class** [[pdf]](https://ieeexplore.ieee.org/document/9747693)</span>  
<span style="font-size:60%"><u>Nakyeong Yang</u>, Taegwan Kang, Kyomin Jung  
[ICASSP 2022 (Oral)](https://2022.ieeeicassp.org/)

<span style="font-size:60%">**[2] Semantic and Explainable Research-related Recommendation System based on Semi-supervised Methodology using BERT and LDA models** [[pdf]](https://www.sciencedirect.com/science/article/abs/pii/S0957417421015232)</span>  
<span style="font-size:60%"><u>Nakyeong Yang</u>, Jeongje Jo, Myeongjun Jeon, Wooju Kim, Juyoung Kang  
[Expert Systems with Applications (2022)](https://www.sciencedirect.com/journal/expert-systems-with-applications)

<span style="font-size:60%">**[1] RABERT: Relation-Aware BERT for Target-Oriented Opinion Words Extraction** [[pdf]](https://dl.acm.org/doi/abs/10.1145/3459637.3482165)</span>  
<span style="font-size:60%">Taegwan Kang, Minwoo Lee, <u>Nakyeong Yang</u>, Kyomin Jung  
[CIKM 2021](https://www.cikm2021.org/)



## ArXiv

<span style="font-size:60%">**[5] Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning** [[pdf]](https://arxiv.org/pdf/2509.22263)</span>  
<span style="font-size:60%"><u>Nakyeong Yang</u>, Dong-kyum Kim, Jea Kwon, Minsung Kim, Kyomin Jung, Meeyoung Cha  
<a style="font-size:60%">Internship at Max Planck Institute for Security and Privacy (MPI-SP)</a>

<span style="font-size:60%">**[4] Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models** [[pdf]](https://www.arxiv.org/pdf/2510.02370)</span>  
<span style="font-size:60%">Minsung Kim, Dong-kyum Kim, Jea Kwon, <u>Nakyeong Yang</u>, Kyomin Jung, Meeyoung Cha  
<a style="font-size:60%">Internship at Max Planck Institute for Security and Privacy (MPI-SP)</a>

<span style="font-size:60%">**[3] Bilinear relational structure fixes reversal curse and enables consistent model editing** [[pdf]](https://arxiv.org/pdf/2509.21993)</span>  
<span style="font-size:60%">Dong-kyum Kim, Minsung Kim, Jea Kwon, <u>Nakyeong Yang</u>, Meeyoung Cha  
<a style="font-size:60%">Internship at Max Planck Institute for Security and Privacy (MPI-SP)</a>

<span style="font-size:60%">**[2] Persona Switch: Mixing Distinct Perspectives in Decoding Time** </span>  
<span style="font-size:60%">Junseok Kim, <u>Nakyeong Yang</u>, Kyomin Jung  

<span style="font-size:60%">**[1] Rethinking Post-Unlearning Behavior of Large Vision-Language Models** [[pdf]](https://arxiv.org/abs/2506.02541)</span>  
<span style="font-size:60%">Minsung Kim, <u>Nakyeong Yang</u>, Kyomin Jung  
