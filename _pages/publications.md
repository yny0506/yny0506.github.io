---
title: "Publications"
permalink: /publications/
layout: single
comments: false
---

## Accepted Papers

<span style="font-size:50%">**[14] Persona Switch: Mixing Distinct Perspectives in Decoding Time** </span>  
<span style="font-size:50%">Junseok Kim, <u>Nakyeong Yang</u>, Kyomin Jung  
[EACL 2026 Findings](https://2026.eacl.org/)</span>  

<span style="font-size:50%">**[13] Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning** [[pdf]](https://arxiv.org/abs/2511.06190)</span>  
<span style="font-size:50%">Sangmook Lee, Dohyung Kim, Hyukhun Koh, <u>Nakyeong Yang</u>, Kyomin Jung  
[AAAI 2026](https://aaai.org/conference/aaai/aaai-26/)</span>  

<span style="font-size:50%">**[12] Persona is a Double-edged Sword: Mitigating the Negative Impact of Role-playing Prompts in Zero-shot Reasoning Tasks** [[pdf]](https://arxiv.org/abs/2408.08631)</span>  
<span style="font-size:50%">Junseok Kim, <u>Nakyeong Yang</u>, Kyomin Jung  
[IJCNLP-AACL 2025 Findings](https://2025.aaclnet.org/)</span>  

<span style="font-size:50%">**[11] FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge** [[pdf]](https://arxiv.org/abs/2502.19207)</span>  
<span style="font-size:50%"><u>Nakyeong Yang</u>, Minsung Kim, Seunghyun Yoon, Joongbo Shin, Kyomin Jung  
[EMNLP 2025](https://2025.emnlp.org)</span>  
<a href="https://research.adobe.com/publication/faithun-toward-faithful-forgetting-in-language-models-by-investigating-the-interconnectedness-of-knowledge/" style="font-size:50%">Joint work with Adobe Research</a>

<span style="font-size:50%">**[10] Avoidance Decoding for Diverse Multi-Branch Story Generation**</span>  
<span style="font-size:50%">Kyeongman Park, <u>Nakyeong Yang</u>, Kyomin Jung  
[EMNLP 2025](https://2025.emnlp.org)</span>

<span style="font-size:50%">**[9] Unplug and Play Language Models: Decomposing Experts in Language Models at Inference Time** [[pdf]](https://arxiv.org/abs/2404.11916)</span>  
<span style="font-size:50%"><u>Nakyeong Yang</u>, Jiwon Moon, Junseok Kim, Yunah Jang, Kyomin Jung  
[CIKM 2025 (Oral)](https://cikm2025.org)</span>

<span style="font-size:50%">**[8] MVMR: A New Framework for Evaluating Faithfulness of Video Moment Retrieval against Multiple Distractors** [[pdf]](https://dl.acm.org/doi/10.1145/3627673.3679838)</span>  
<span style="font-size:50%"><u>Nakyeong Yang</u>, Minsung Kim, Seunghyun Yoon, Joongbo Shin, Kyomin Jung  
[CIKM 2024 (Oral)](https://cikm2024.org/)</span>  
<a href="https://research.adobe.com/publication/mvmr-a-new-framework-for-evaluating-faithfulness-of-video-moment-retrieval-against-multiple-distractors/" style="font-size:50%">Joint work with Adobe Research</a>

<span style="font-size:50%">**[7] Mitigating Biases for Instruction-following Language Models via Bias Neurons Elimination** [[pdf]](https://aclanthology.org/2024.acl-long.490/)</span>  
<span style="font-size:50%"><u>Nakyeong Yang</u>, Taegwan Kang, Jungkyu Choi, Honglak Lee, Kyomin Jung  
[ACL 2024](https://2024.aclweb.org/)</span>  
<a href="https://www.lgresearch.ai/publication/view?seq=110" style="font-size:50%">Internship at LG AI Research</a>

<span style="font-size:50%">**[6] LongStory: Coherent, Complete and Length Controlled Long story Generation** [[pdf]](https://arxiv.org/abs/2311.15208)</span>  
<span style="font-size:50%">Kyeongman Park, <u>Nakyeong Yang</u>, Kyomin Jung  
[PAKDD 2024 (Oral)](https://pakdd2024.org/)</span>

<span style="font-size:50%">**[5] Task-specific Compression for Multi-task Language Models using Attribution-based Pruning** [[pdf]](https://aclanthology.org/2023.findings-eacl.43/)</span>  
<span style="font-size:50%"><u>Nakyeong Yang</u>, Yunah Jang, Hwanhee Lee, Seohyeong Jeong, Kyomin Jung  
[EACL 2023 Findings](https://2023.eacl.org/)</span>

<span style="font-size:50%">**[4] Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer** [[pdf]](https://arxiv.org/abs/2303.13099)</span>  
<span style="font-size:50%">Hyukhun Koh, Haesung Pyun, <u>Nakyeong Yang</u>, Kyomin Jung  
[SIGDial 2023 Workshop (DSTC 11)](https://dstc11.dstc.community/)</span>

<span style="font-size:50%">**[3] Deriving Explainable Discriminative Attributes Using Confusion About Counterfactual Class** [[pdf]](https://ieeexplore.ieee.org/document/9747693)</span>  
<span style="font-size:50%"><u>Nakyeong Yang</u>, Taegwan Kang, Kyomin Jung  
[ICASSP 2022 (Oral)](https://2022.ieeeicassp.org/)</span>

<span style="font-size:50%">**[2] Semantic and Explainable Research-related Recommendation System based on Semi-supervised Methodology using BERT and LDA models** [[pdf]](https://www.sciencedirect.com/science/article/abs/pii/S0957417421015232)</span>  
<span style="font-size:50%"><u>Nakyeong Yang</u>, Jeongje Jo, Myeongjun Jeon, Wooju Kim, Juyoung Kang  
[Expert Systems with Applications (2022)](https://www.sciencedirect.com/journal/expert-systems-with-applications)</span>

<span style="font-size:50%">**[1] RABERT: Relation-Aware BERT for Target-Oriented Opinion Words Extraction** [[pdf]](https://dl.acm.org/doi/abs/10.1145/3459637.3482165)</span>  
<span style="font-size:50%">Taegwan Kang, Minwoo Lee, <u>Nakyeong Yang</u>, Kyomin Jung  
[CIKM 2021](https://www.cikm2021.org/)</span>



## ArXiv

<span style="font-size:50%">**[5] Reliability-Aware Adaptive Self-Consistency for Efficient Sampling in LLM Reasoning** [[pdf]](https://arxiv.org/pdf/2601.02970)</span>  
<span style="font-size:50%">Junseok Kim, <u>Nakyeong Yang</u>, Kyungmin Min, Kyomin Jung</span>  

<span style="font-size:50%">**[4] Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning** [[pdf]](https://arxiv.org/pdf/2509.22263)</span>  
<span style="font-size:50%"><u>Nakyeong Yang</u>, Dong-kyum Kim, Jea Kwon, Minsung Kim, Kyomin Jung, Meeyoung Cha</span>  
<a href="https://www.mpi-sp.org/" style="font-size:50%">Internship at Max Planck Institute for Security and Privacy (MPI-SP)</a>

<span style="font-size:50%">**[3] Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models** [[pdf]](https://www.arxiv.org/pdf/2510.02370)</span>  
<span style="font-size:50%">Minsung Kim, Dong-kyum Kim, Jea Kwon, <u>Nakyeong Yang</u>, Kyomin Jung, Meeyoung Cha</span>  
<a href="https://www.mpi-sp.org/" style="font-size:50%">Internship at Max Planck Institute for Security and Privacy (MPI-SP)</a>

<span style="font-size:50%">**[2] Bilinear relational structure fixes reversal curse and enables consistent model editing** [[pdf]](https://arxiv.org/pdf/2509.21993)</span>  
<span style="font-size:50%">Dong-kyum Kim, Minsung Kim, Jea Kwon, <u>Nakyeong Yang</u>, Meeyoung Cha</span>  
<a href="https://www.mpi-sp.org/" style="font-size:50%">Internship at Max Planck Institute for Security and Privacy (MPI-SP)</a>

<span style="font-size:50%">**[1] Rethinking Post-Unlearning Behavior of Large Vision-Language Models** [[pdf]](https://arxiv.org/abs/2506.02541)</span>  
<span style="font-size:50%">Minsung Kim, <u>Nakyeong Yang</u>, Kyomin Jung</span>
