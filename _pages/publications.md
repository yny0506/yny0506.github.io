---
title: "Publications"
permalink: /publications/
layout: single
comments: false
---
<head>
  <style>
    .each_div {
      line-height: 0.95;
      margin-bottom: 15px;
    }

    .title {
      font-size: 60%;
      font-style: bold;
    }

    .content {
      font-size: 50%;
    }
  </style>
</head>



## Accepted Papers
<div class="each_div">
  <span class="title"><strong>[16] Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning</strong> <a href="https://arxiv.org/pdf/2509.22263">[pdf]</a></span><br>  
  <span class="content"><i><u>Nakyeong Yang</u>, Dong-kyum Kim, Jea Kwon, Minsung Kim, Kyomin Jung, Meeyoung Cha</i></span><br>  
  <a class="content" href="https://iclr.cc/"><strong>ICLR 2026</strong></a><br>
  <a class="content" href="https://www.mpi-sp.org/"><strong>Internship at Max Planck Institute for Security and Privacy (MPI-SP)</strong></a>
</div>

<div class="each_div">
  <span class="title"><strong>[15] Bilinear relational structure fixes reversal curse and enables consistent model editing</strong> <a href="https://arxiv.org/pdf/2509.21993">[pdf]</a></span><br>  
  <span class="content"><i>Dong-kyum Kim, Minsung Kim, Jea Kwon, <u>Nakyeong Yang</u>, Meeyoung Cha</i></span><br>  
  <a class="content" href="https://iclr.cc/"><strong>ICLR 2026</strong></a><br>
  <a class="content" href="https://www.mpi-sp.org/"><strong>Internship at Max Planck Institute for Security and Privacy (MPI-SP)</strong></a>
</div>

<div class="each_div">
  <span class="title"><strong>[14] Persona Switch: Mixing Distinct Perspectives in Decoding Time</strong> <a href="https://arxiv.org/abs/2601.15708">[pdf]</a></span><br>  
  <span class="content"><i>Junseok Kim, <u>Nakyeong Yang</u>, Kyomin Jung</i></span><br>  
  <a class="content" href="https://2026.eacl.org/"><strong>EACL 2026 Findings</strong></a><br>
</div>

<div class="each_div">
  <span class="title"><strong>[13] Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning</strong> <a href="https://arxiv.org/abs/2511.06190">[pdf]</a></span><br>  
  <span class="content"><i>Sangmook Lee, Dohyung Kim, Hyukhun Koh, <u>Nakyeong Yang</u>, Kyomin Jung</i></span><br>  
  <a class="content" href="https://aaai.org/conference/aaai/aaai-26/"><strong>AAAI 2026</strong></a><br>
</div>

<div class="each_div">
  <span class="title"><strong>[12] Persona is a Double-edged Sword: Mitigating the Negative Impact of Role-playing Prompts in Zero-shot Reasoning Tasks</strong> <a href="https://arxiv.org/abs/2408.08631">[pdf]</a></span><br>  
  <span class="content"><i>Junseok Kim, <u>Nakyeong Yang</u>, Kyomin Jung</i></span><br>  
  <a class="content" href="https://2025.aaclnet.org/"><strong>IJCNLP-AACL 2025 Findings</strong></a><br>
</div>

<div class="each_div">
  <span class="title"><strong>[11] FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge</strong> <a href="https://arxiv.org/abs/2502.19207">[pdf]</a> <a href="https://github.com/yny0506/faithun">[data]</a></span><br>  
  <span class="content"><i><u>Nakyeong Yang</u>, Minsung Kim, Seunghyun Yoon, Joongbo Shin, Kyomin Jung</i></span><br>  
  <a class="content" href="https://2025.emnlp.org"><strong>EMNLP 2025</strong></a><br>
  <a class="content" href="https://research.adobe.com/publication/faithun-toward-faithful-forgetting-in-language-models-by-investigating-the-interconnectedness-of-knowledge/"><strong>Joint work with Adobe Research</strong></a>
</div>

<div class="each_div">
  <span class="title"><strong>[10] Avoidance Decoding for Diverse Multi-Branch Story Generation <a href="https://aclanthology.org/2025.emnlp-main.381/">[pdf]</a></span><br>  
  <span class="content"><i>Kyeongman Park, <u>Nakyeong Yang</u>, Kyomin Jung</i></span><br>  
  <a class="content" href="https://2025.emnlp.org"><strong>EMNLP 2025</strong></a><br>
</div>

<div class="each_div">
  <span class="title"><strong>[9] Unplug and Play Language Models: Decomposing Experts in Language Models at Inference Time <a href="https://arxiv.org/abs/2404.11916">[pdf]</a></span><br>  
  <span class="content"><i><u>Nakyeong Yang</u>, Jiwon Moon, Junseok Kim, Yunah Jang, Kyomin Jung</i></span><br>  
  <a class="content" href="https://cikm2025.org"><strong>CIKM 2025 (Oral)</strong></a><br>
</div>

<div class="each_div">
  <span class="title"><strong>[8] MVMR: A New Framework for Evaluating Faithfulness of Video Moment Retrieval against Multiple Distractors <a href="https://dl.acm.org/doi/10.1145/3627673.3679838">[pdf]</a> <a href="https://github.com/yny0506/Massive-Videos-Moment-Retrieval">[data]</a></span><br>  
  <span class="content"><i><u>Nakyeong Yang</u>, Minsung Kim, Seunghyun Yoon, Joongbo Shin, Kyomin Jung</i></span><br>  
  <a class="content" href="https://cikm2024.org/"><strong>CIKM 2024 (Oral)</strong></a><br>
  <a class="content" href="https://research.adobe.com/publication/faithun-toward-faithful-forgetting-in-language-models-by-investigating-the-interconnectedness-of-knowledge/"><strong>Joint work with Adobe Research</strong></a>
</div>

<div class="each_div">
  <span class="title"><strong>[7] Mitigating Biases for Instruction-following Language Models via Bias Neurons Elimination <a href="https://aclanthology.org/2024.acl-long.490/">[pdf]</a></span><br>  
  <span class="content"><i><u>Nakyeong Yang</u>, Taegwan Kang, Jungkyu Choi, Honglak Lee, Kyomin Jung</i></span><br>  
  <a class="content" href="https://2024.aclweb.org/"><strong>ACL 2024</strong></a><br>
  <a class="content" href="https://www.lgresearch.ai/publication/view?seq=110"><strong>Internship at LG AI Research</strong></a>
</div>

<div class="each_div">
  <span class="title"><strong>[6] LongStory: Coherent, Complete and Length Controlled Long story Generation <a href="https://arxiv.org/abs/2311.15208">[pdf]</a></span><br>  
  <span class="content"><i>Kyeongman Park, <u>Nakyeong Yang</u>, Kyomin Jung</i></span><br>  
  <a class="content" href="https://pakdd2024.org/"><strong>PAKDD 2024 (Oral)</strong></a><br>
</div>

<div class="each_div">
  <span class="title"><strong>[5] Task-specific Compression for Multi-task Language Models using Attribution-based Pruning <a href="https://aclanthology.org/2023.findings-eacl.43/">[pdf]</a></span><br>  
  <span class="content"><i><u>Nakyeong Yang</u>, Yunah Jang, Hwanhee Lee, Seohyeong Jeong, Kyomin Jung</i></span><br>  
  <a class="content" href="https://2023.eacl.org/"><strong>EACL 2023 Findings</strong></a><br>
</div>

<div class="each_div">
  <span class="title"><strong>[4] Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer <a href="https://arxiv.org/abs/2303.13099">[pdf]</a></span><br>  
  <span class="content"><i>Hyukhun Koh, Haesung Pyun, <u>Nakyeong Yang</u>, Kyomin Jung</i></span><br>  
  <a class="content" href="https://dstc11.dstc.community/"><strong>SIGDial 2023 Workshop (DSTC 11)</strong></a><br>
</div>

<div class="each_div">
  <span class="title"><strong>[3] Deriving Explainable Discriminative Attributes Using Confusion About Counterfactual Class <a href="https://ieeexplore.ieee.org/document/9747693">[pdf]</a></span><br>  
  <span class="content"><i><u>Nakyeong Yang</u>, Taegwan Kang, Kyomin Jung</i></span><br>  
  <a class="content" href="https://2022.ieeeicassp.org/"><strong>ICASSP 2022 (Oral)</strong></a><br>
</div>

<div class="each_div">
  <span class="title"><strong>[2] Semantic and Explainable Research-related Recommendation System based on Semi-supervised Methodology using BERT and LDA models <a href="https://www.sciencedirect.com/science/article/abs/pii/S0957417421015232">[pdf]</a></span><br>  
  <span class="content"><i><u>Nakyeong Yang</u>, Jeongje Jo, Myeongjun Jeon, Wooju Kim, Juyoung Kang</i></span><br>  
  <a class="content" href="https://www.sciencedirect.com/journal/expert-systems-with-applications"><strong>Expert Systems with Applications (2022)</strong></a><br>
</div>

<div class="each_div">
  <span class="title"><strong>[1] RABERT: Relation-Aware BERT for Target-Oriented Opinion Words Extraction <a href="https://dl.acm.org/doi/abs/10.1145/3459637.3482165">[pdf]</a></span><br>  
  <span class="content"><i>Taegwan Kang, Minwoo Lee, <u>Nakyeong Yang</u>, Kyomin Jung</i></span><br>  
  <a class="content" href="https://www.cikm2021.org/"><strong>CIKM 2021</strong></a><br>
</div>



## ArXiv

<div class="each_div">
  <span class="title"><strong>[3] Reliability-Aware Adaptive Self-Consistency for Efficient Sampling in LLM Reasoning <a href="https://arxiv.org/pdf/2601.02970">[pdf]</a></span><br>  
  <span class="content"><i>Junseok Kim, <u>Nakyeong Yang</u>, Kyungmin Min, Kyomin Jung</i></span><br>  
</div>

<div class="each_div">
  <span class="title"><strong>[2] Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models <a href="https://www.arxiv.org/pdf/2510.02370">[pdf]</a></span><br>  
  <span class="content"><i>Minsung Kim, Dong-kyum Kim, Jea Kwon, <u>Nakyeong Yang</u>, Kyomin Jung, Meeyoung Cha</i></span><br>  
  <a class="content" href="https://www.mpi-sp.org/"><strong>Internship at Max Planck Institute for Security and Privacy (MPI-SP)</strong></a>
</div>

<div class="each_div">
  <span class="title"><strong>[1] Rethinking Post-Unlearning Behavior of Large Vision-Language Models <a href="https://arxiv.org/abs/2506.02541">[pdf]</a></span><br>  
  <span class="content"><i>Minsung Kim, <u>Nakyeong Yang</u>, Kyomin Jung</i></span><br>  
</div>
